@online{cheng2024survey,
  author    = {Cheng, Hongrong and Zhang, Miao and Shi, Javen Qinfeng},
  title     = {A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations},
  year      = {2024},
  url       = {https://arxiv.org/pdf/2308.06767},
  note      = {Version 2 vom 09.08.2024, abgerufen am 08.02.2025}
}

@online{clark2019boolq,
  author    = {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  title     = {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  year      = {2019},
  url       = {https://arxiv.org/pdf/1905.10044},
  note      = {vom 24.05.2019, entnommen am 21.02.2025}
}

@online{clark2019arc,
  author    = {Clark, Peter and Cowhey, Isaak and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  year      = {2019},
  url       = {https://arxiv.org/pdf/1803.05457},
  note      = {vom 14.03.2018, abgerufen am 21.02.2025}
}

@online{he2023structured,
  author    = {He, Yang and Xiao, Lingao},
  title     = {Structured Pruning for Deep Convolutional Neural Networks: A Survey},
  year      = {2023},
  url       = {https://arxiv.org/pdf/2303.00566},
  note      = {Version 2 vom 30.11.2023, abgerufen am 30.01.2025}
}

@online{hu2024slm,
  author    = {Hu, Mengya Mia and Xu, Rui and Lei, Deren and Li, Yaxi and Wang, Mingyu and Ching, Emily and Kamal, Eslam and Deng, Alex},
  title     = {SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination Detection},
  year      = {2024},
  url       = {https://arxiv.org/pdf/2408.12748},
  note      = {Version 1 vom 22.08.2024, abgerufen am 29.01.2025}
}

@online{huang2024hallucination,
  author    = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  title     = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
  year      = {2024},
  url       = {https://arxiv.org/pdf/2311.05232},
  note      = {Version 2 vom 19.11.2024, abgerufen am 29.01.2025}
}

@online{kelbert2023llm,
  author    = {Kelbert, Patricia and Siebert, Julien and Jöckel, Lisa},
  title     = {Was sind Large Language Models? Und was ist bei der Nutzung von KI-Sprachmodellen zu beachten?},
  year      = {2023},
  url       = {https://www.iese.fraunhofer.de/blog/large-language-models-ki-sprachmodelle/},
  note      = {abgerufen am 29.01.2025}
}

@online{li2022random,
  author    = {Li, Yawei and Adamczewski, Kamil and Li, Wen and Gu, Shuhang and Timofte, Radu and Van Gool, Luc},
  title     = {Revisiting Random Channel Pruning for Neural Network Compression},
  year      = {2022},
  url       = {https://arxiv.org/pdf/2205.05676},
  note      = {vom 11.05.2022, abgerufen am 02.02.2025}
}

@online{li2022random2,
  author    = {Li, Yawei and Adamczewski, Kamil and Li, Wen and Gu, Shuhang and Timofte, Radu and Van Gool, Luc},
  title     = {Revisiting Random Channel Pruning for Neural Network Compression},
  year      = {2022},
  url       = {https://arxiv.org/pdf/2205.05676},
  note      = {vom 11.05.2022, abgerufen am 08.02.2025}
}

@online{lu2024taxonomy,
  author    = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Liu, Yue and Xing, Zhenchang and Whittle, Jon},
  title     = {A TAXONOMY OF FOUNDATION MODEL BASED SYSTEMS THROUGH THE LENS OF SOFTWARE ARCHITECTURE},
  year      = {2024},
  url       = {https://arxiv.org/pdf/2305.05352v6},
  note      = {Version 1 vom 22.01.2024, abgerufen am 29.01.2025}
}

@online{mihaylov2018armor,
  author    = {Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  title     = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  year      = {2018},
  url       = {https://export.arxiv.org/pdf/1809.02789v1.pdf},
  note      = {vom 08.09.2018, abgerufen am 22.02.2025}
}

@online{minaee2024survey,
  author    = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  title     = {Large Language Models: A Survey},
  year      = {2024},
  url       = {https://arxiv.org/pdf/2402.06196},
  note      = {Version 6 vom 20.02.2024, abgerufen am 29.01.2025}
}

@online{naveed2024overview,
  author    = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saquib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  title     = {A Comprehensive Overview of Large Language Models},
  year      = {2024},
  url       = {https://arxiv.org/pdf/2307.06435},
  note      = {Version 10 vom 17.10.2024, abgerufen am 29.01.2025}
}

@online{sakaguchi2019winogrande,
  author    = {Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavaula, Chandra and Choi, Yejin},
  title     = {WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale},
  year      = {2019},
  url       = {https://arxiv.org/pdf/1907.10641},
  note      = {Version 2 vom 21.11.2019, abgerufen am 22.02.2025}
}

@article{sakai2022structured,
  author    = {Sakai, Yasufumi and Eto, Yu and Teranishi, Yuta},
  title     = {Structured Pruning for Deep Neural Networks with Adaptive Pruning Rate Derivation Based on Connection Sensitivity and Loss Function},
  journal   = {Journal of Advances in Information Technology},
  volume    = {13},
  number    = {3},
  year      = {2022},
  pages     = {295--300}
}

@article{vadera2022methods,
  author    = {Vadera, Sunil and Ameen, Salem},
  title     = {Methods for Pruning Deep Neural Networks},
  journal   = {IEEE Access},
  volume    = {10},
  year      = {2022},
  url       = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9795013},
  note      = {abgerufen am 18.02.2025}
}

@online{wang2025adapt,
  author    = {Wang, Boyao and Pan, Rui and Diao, Shizhe and Pan, Xingyuan and Zhang, Jipeng and Pi, Renjie and Zhang, Tong},
  title     = {Adapt-Pruner: Adaptive structural pruning for efficient small language Model Training},
  year      = {2025},
  url       = {https://arxiv.org/pdf/2502.03460v1},
  note      = {vom 05.02.2025, abgerufen am 08.02.2025}
}

@online{zellers2019hellaswag,
  author    = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  title     = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  year      = {2019},
  url       = {https://arxiv.org/pdf/1905.07830},
  note      = {vom 19.05.2019, abgerufen am 21.02.2021}
}

@online{zhu2022prune,
  author    = {Zhu, Michael and Gupta, Suyog},
  title     = {To prune, or not to prune: exploring the efficacy of pruning for model compression},
  year      = {2022},
  url       = {https://arxiv.org/pdf/1710.01878},
  note      = {Version 2 vom 13.11.2017, abgerufen am 08.02.2025}
}

@online{ptflops,
  author = {Vladislav Sovrasov},
  title = {ptflops: Ein Tool zur Berechnung von FLOPs für neuronale Netze im PyTorch-Framework},
  year = 2018,
  url = {https://pypi.org/project/ptflops/},
}

@article{zhang2024tinyllama,
  author = {Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
  title = {TinyLlama: An Open-Source Small Language Model},
  year = {2024},
  journal = {arXiv},
  volume = {2401.02385},
  version = {2},
  url = {https://arxiv.org/pdf/2401.02385},
  note = {Version 2 vom 04.06.2024, abgerufen am 23.02.2025}
}

@misc{zhang2023eval,
  author = {Zhang, Peiyuan},
  title = {Evaluate TinyLlama},
  year = {2023},
  url = {https://github.com/jzhang38/TinyLlama/blob/main/EVAL.md},
  note = {Abgerufen am 23.02.2025, vom 28.12.2023}
}

@article{zhu2017pruning,
  author = {Zhu, Michael and Gupta, Suyog},
  title = {To prune, or not to prune: exploring the efficacy of pruning for model compression},
  year = {2017},
  journal = {arXiv},
  volume = {1710.01878},
  version = {2},
  url = {https://arxiv.org/pdf/1710.01878},
  note = {Version 2 vom 13.11.2017, abgerufen am 08.02.2025}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{llmpruner,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{wanda,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@inproceedings{bertgoogle,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@online{gpt4size,
  author    = {Maximilian Schreiner},
  title     = {{GPT-4 architecture, datasets, costs and more leaked}},
  year      = {2023},
  url       = {https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/},
  urldate   = {2025-02-24},
  note      = {Veröffentlicht auf THE DECODER am 11. Juli 2023}
}
