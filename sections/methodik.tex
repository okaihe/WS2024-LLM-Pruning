\section{Methodik}\label{methodik}

\subsection{Literaturrecherche}

\subsection{Arbeitsorganisation}

Für die Zusammenarbeit und Organisation der anstehenden Aufgaben und zu
verfassenden Kapitel wurde das Notizenprogramm \emph{Notion} verwendet. Notion
erlaubt das Erstellen kollaborativer Notizbücher, die parallel von mehreren
Personen bearbeitet werden können. Über die Datenbankfunktion lassen sich sowohl
Meilensteinpläne als auch Task-Boards erstellen, die zur Planung der Arbeit
verwendet wurden.

\subsection{Entwicklungsumgebung}

Da sowohl das Pruning von Large Language Models als auch die anschließende
Evaluierung der geprunten Modelle erhebliche Rechenressourcen erfordern, wurde
entschieden, diese Prozesse auf einem leistungsstarken Server in der Cloud
durchzuführen. Insbesondere das Testen der resultierenden Modelle stellt eine
hohe Belastung für die verfügbaren Ressourcen dar und ist auf herkömmlicher
Hardware nicht effizient durchzuführen. Zu diesen Zweck wurden EC2-Instanzen des
Cloud-Providers AWS (Amazon Web Services) aus der \emph{g}-Instanzfamilie
verwendet. Diese Instanzfamilie ist speziell auf rechenintensive Anwendungen
ausgelegt und bietet GPUs (Graphics Processing Units), die für parallele
Berechnungen optimiert sind und den Einsatz von CUDA, dem von \emph{NVIDIA}
entwickelten Toolkit zur Verwendung von GPUs in allgemeinen Rechenaufgaben,
ermöglichen.

Im Detail wurden Instanzen des Typs \emph{gd4n.xlarge} verwendet, die mit 16 GiB
Hauptspeicher, einem leistungsstarken Prozessor der \emph{Intel Xeon Family}
sowie einer \emph{NVIDIA T4 Tensor Core} Grafikeinheit ausgestattet sind. Für
den Start der Instanzen wurde das speziell auf Deep-Learning-Anwendungen
zugeschnittene \emph{Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.5.1
	(Ubuntu 22.04) 20241208} Ubuntu-Image verwendet. In diesem Image ist das
benötigte Python-Package \emph{PyTorch} bereits vorinstalliert und ist
zusätzlich mit \emph{CUDA} ausgestattet.

Verwendet wurden im Detail Instanzen des Types \emph{gd4n.xlarge}, die mit 16GiB
Hauptspeicher, einem physischen Prozessor der \emph{Intel Xeon Family} und einer
\emph{NVIDIA T4 Tensor Core} Grafikeinheit kommen. Gestartet wurden die
Instanzen mit dem \emph{Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.5.1
	(Ubuntu 22.04) 20241208} Ubuntu Image, das bereits das benötigte Python-Package
\emph{PyTorch} vorinstalliert hat. Diese, auf Deep Learning ausgerichteten,
Images haben neben PyTorch bereits \emph{CUDA} vorinstalliert und ermöglichen so
einen einfachen Start in das Arbeiten mit Programmen, die die GPU benötigen.

\begin{table}[ht!]
	\centering
	\begin{tabular}{|p{3cm}|p{12cm}|}
		\hline
		\multicolumn{2}{|c|}{\textbf{Verwendete EC2-Instanz}}                 \\
		\hline
		Instanz-Typ   & gd4n.xlarge                                           \\
		\hline
		Hauptspeicher & 16GiB                                                 \\
		\hline
		vCPU          & 4                                                     \\
		\hline
		Clock Speed   & 2.5GHz                                                \\
		\hline
		GPU           & nvidia t4 tensor core                                 \\
		\hline
		CPU           & Intel Xeon Family                                     \\
		\hline
		AMI           & Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.5.1 \\
		\hline
		AMI-ID        & ami-0fa5e5fd27b3e163a                                 \\
		\hline
	\end{tabular}
	\caption{Attribute der verwendeten Hardware}
\end{table}

Nachdem über SSH (Secure Shell) mit der jeweiligen Instanz verbunde wurde,
wurde zunächst die vorinstallierte PyTorch-Umgebung gestartet und
anschließenden das Repository des verwendeten Frameworks geklont.

\vspace{1em}
\begin{lstlisting}
$ source activate pytorch
$ git clone https://github.com/horseee/LLM-Pruner.git
\end{lstlisting}

Da die auf dem Image vorinstallierte PyTorch-Umgebung mit dem
Conda-Package-Manager bereitgestellt wird, wurden die im Repository in der
\emph{requirements.txt}-Datei angebenen Package über Conda installiert.

\vspace{1em}
\begin{lstlisting}
$ conda install transformers sentencepiece datasets wandb ...
\end{lstlisting}

Damit ist die Versuchsumgebung identisch zu der in dieser Arbeit verwendeten
Umgebung.

\subsection{Verwendetes Large Language Model}

Aufgrund von finanziellen und zeitlichen Einschränkungen wurde für diese Analyse
das TinyLlama-Modell (\emph{TinyLlama/TinyLlama-1.1B-Chat-v1.0}) als Basismodell
gewählt. Mit 1,1 Milliarden Parametern ist es im Vergleich zu modernen, größeren
Modellen wie \emph{Llama 3.1} – das mit 405 Milliarden Parametern deutlich
umfangreicher ist – relativ klein. Das verwendete TinyLlama wurde auf einem
Datensatz von drei Milliarden Token vortrainiert und basiert auf der gleichen
Architektur wie die \emph{Llama 2}-Modelle. Diese Wahl ermöglichte es, innerhalb
der verfügbaren Ressourcen eine Analyse durchzuführen, während gleichzeitig die
Komplexität des Modells berücksichtigt wurde.

Der nachfolgende Auszug zeigt die detailierte Architektur des Modells:

\vspace{1em}
\begin{lstlisting}
LlamaModel(
  (embed_tokens): Embedding(32000, 2048)
  (layers): ModuleList(
    (0-21): 22 x LlamaDecoderLayer(
      (self_attn): LlamaSdpaAttention(
        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (k_proj): Linear(in_features=2048, out_features=256, bias=False)
        (v_proj): Linear(in_features=2048, out_features=256, bias=False)
        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
        (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
    )
  )
  (norm): LlamaRMSNorm((2048,), eps=1e-05)
  (rotary_emb): LlamaRotaryEmbedding()
)
\end{lstlisting}

Die Architektur besteht aus einer Embedding-Schicht, über die die
Eingabesequenzen in Vektoren der Dimension 2048 umwandelt werden. Die
Token-Embeddings basieren auf einem Vokabular von 32.000 Wörtern. Darauf folgt
eine Modul-Liste mit 22 LlamaDecoderLayer, die jeweils aus mehreren Submodulen
bestehen. Die Decoder-Schicht enthält einen Self-Attention-Mechanismus
(LlamaSdpaAttention).

Um die grundlegenden Funktionen der ersten und letzten Schichten nicht zu
beeinflussen, wurden sowohl im Attention-Abschnitt als auch im MLP-Abschnitt
ausschließlich die Layer 4 bis 18 für das Pruning verwendet. Die Schichten der
Architektur außerhalb des Decoder-Layers wie bspw. die Embedding-Schichten
werden grundsätzlich nicht berührt, da sie für die Umwandlung der Texteingaben
in die korrekte Repräsentation nötig sind.

\subsection{Pruning}

Das nachfolgende Kapitel beinhaltet die Vorgehensweise und verwendeten
Technologien, die für das Pruning des TinyLlama-Modells verwendet wurde.

\subsubsection{Verwendetes Framework}

Für die Durchführung des Prunings standen zwei Frameworks zur Auswahl:
\emph{LLM-Pruner} und \emph{Wanda} (Pruning by Weights and Activations). Während
der \emph{LLM-Pruner} ausschließlich das im vorherigen Kapitel beschriebene
strukturierte Pruning unterstützt, bietet \emph{Wanda} zusätzlich die
Möglichkeit, unstrukturiertes Pruning durchzuführen. Trotz dieser zusätzlichen
Funktionalität wurde für die Umsetzung dieser Untersuchung aus verschiedenen
Gründen ausschließlich der \emph{LLM-Pruner} verwendet.

Ein wesentlicher Faktor für diese Entscheidung war die Aktualität der Projekte.
Die letzten Updates im \emph{Wanda}-Projekt wurden Ende 2023 vorgenommen. Dies
lag zum Zeitpunkt des Verfassens dieser Arbeit bereits mehr als ein Jahr zurück.
Im Gegensatz dazu wird der \emph{LLM-Pruner} weiterhin aktiv weiterentwickelt,
was eine aktuellere und besser unterstützte Basis bietet. Ein weiterer wichtiger
Aspekt ist die Unterstützung spezifischer Modelle. Während beim
\emph{LLM-Pruner} explizit die Kompatibilität mit dem \emph{TinyLlama}-Modell
hervorgehoben wird, fehlt eine solche Aussage im Fall von \emph{Wanda}. Es ist
zwar anzunehmen, dass die Methoden von \emph{Wanda} aufgrund der Unterstützung
von \emph{Llama-2} – dessen Architektur dem \emph{TinyLlama} ähnlich ist –
ebenfalls auf das \emph{TinyLlama}-Modell anwendbar sein könnten. Allerdings
bleibt dies ungewiss, da eine direkte Unterstützung nicht garantiert wird.

Ein weiterer entscheidender Punkt war, dass trotz mehrerer Versuche mit
\emph{Wanda} kein erfolgreiches Pruning durchgeführt werden konnte. Angesichts
dieser Schwierigkeiten und unter Berücksichtigung der bereits genannten
Argumente fiel die Entscheidung, sich ausschließlich auf den \emph{LLM-Pruner}
zu fokussieren.

\subsubsection{Durchführung des Prunings}

Das Pruning wurde, wie bereits erläutert, anhand des \emph{TinyLlama}-Modells
getestet und untersucht. Dabei wurden drei unterschiedliche Pruning-Ratios
gewählt, um die Auswirkungen verschiedener Verkleinerungsgrade des Modells zu
analysieren. Da das \emph{LLM-Pruner}-Projekt bereits eigene Ergebnisse für das
\emph{TinyLlama}-Modell mit einer Pruning-Ratio von 20\% veröffentlicht hatte,
wurde dieses Verhältnis in der vorliegenden Untersuchung nicht erneut evaluiert.
Stattdessen wurden die bereits existierenden Ergebnisse mit den Verhältnissen
von 30\%, 40\% und schließlich 70\% verglichen. Obwohl eine Verkleinerung um 70\%
bei einem ohnehin bereits kompakten Modell wie \emph{TinyLlama} als
unrealistisch erscheint, wurde dennoch im Rahmen der Untersuchung überprüft,
welche Ergebnisse das Modell bei einer solch extremen Reduktion in den Tests
liefert.

Das Pruning erfolgt stets über den folgenden Befehl, der ausgeführt werden kann,
sobald sich im LLM-Pruner-Repository auf der höchsten Ebene befindet:

\vspace{1em}
\begin{lstlisting}
$ python llama3.py
    --base_model TinyLlama/TinyLlama-1.1B-Chat-v1.0
    --pruning_ratio [PRUNING_RATIO]
    --device cuda
    --eval_device cuda
    --block_wise
    --block_mlp_layer_start [START_MLP_LAYER]
    --block_mlp_layer_end [END_MLP_LAYER]
    --block_attention_layer_start [START_ATTENTION_LAYER]
    --block_attention_layer_end [END_ATTENTION_LAYER]
    --save_ckpt_log_name [SAVE_PATH]
    --pruner_type [PRUNER_TYPE]
    [--taylor param_first]
    --save_model
    --max_seq_len 2048
    --test_after_train
\end{lstlisting}

Für das Pruning des \emph{TinyLlama}-Modells wurde das Skript \emph{llama3.py}
verwendet, das speziell für das Pruning von \emph{Llama3}-Modellen entwickelt
wurde. In dieser Untersuchung diente stets das zuvor beschriebene Modell
\emph{TinyLlama/TinyLlama-1.1B-Chat-v1.0} als \emph{base\_model}. Wie bereits
erwähnt, wurden für die Pruning-Ratio die Verhältnisse 0.3, 0.5 und 0.7
ausgewählt, um unterschiedliche Stufen der Modellreduktion zu analysieren.

Alle Prozesse wurden in einer CUDA-fähigen Umgebung ausgeführt, weshalb die
Anweisung, die GPU für die Berechnungen zu verwenden, stets mitgegeben wurde.
Von den insgesamt 22 verfügbaren MLP- und Attention-Layern des Modells wurden
für das Pruning jeweils die Layer vier bis 18 berücksichtigt.

Bezüglich der verfügbaren Pruner-Typen bot der \emph{LLM-Pruner} vier
verschiedene Optionen an: \emph{Taylor}, \emph{L1}, \emph{L2} und \emph{random}.
Jede dieser vier Methoden wurde für jede der drei gewählten Pruning-Ratios
getestet und evaluiert, um ihre jeweiligen Auswirkungen auf die Modellleistung
zu untersuchen.

Zusätzlich wurde bei allen Experimenten das Argument \emph{\-\-test\_after\_train}
verwendet. Dadurch wurde nach jedem Pruning automatisch die Perplexity des
Modells ermittelt, basierend auf den beiden Testdatensätzen \emph{wikidata2} und
\emph{ptb} (\emph{Penn Treebank}).

Um das TinyLlama-Modell zu 30\% mit der \emph{Taylor}-Methode zu prunen sieht
der Befehl demnach beispielhaft wie folgt aus:

\vspace{1em}
\begin{lstlisting}
$ python llama3.py 
    --base_model TinyLlama/TinyLlama-1.1B-Chat-v1.0
    --pruning_ratio 0.3
    --device cuda
    --eval_device cuda
    --block_wise
    --block_mlp_layer_start 4
    --block_mlp_layer_end 18
    --block_attention_layer_start 4
    --block_attention_layer_end 18
    --save_ckpt_log_name tinyllama_30_0616_prune_log
    --pruner_type taylor
    --taylor param_first
    --save_model
    --max_seq_len 2048
    --test_after_train
\end{lstlisting}

\subsection{Fine-Tuning}

Für das nach dem Pruning stattfindende Fine-Tuning wird vom LLM-Pruner
\emph{PEFT} (Parameter-Efficient Fine-Tuning) verwendet. PEFT stellt eine
Methode dar, um große vortrainierte Modelle an spezifische Aufgaben anzupassen,
ohne den gesamten Parameterraum des Modells zu optimieren. Stattdessen wird nur
ein kleinerer Teil der Parameter während des Trainings modifiziert.

In den Evaluierungen der Modelle, die direkt im Anschluss an das Pruning
durchgeführt wurden, hat sich bereits gezeigt, dass über die Pruning-Methode
\emph{Taylor} die vielversprechendsten Ergebnisse erzielt werden konnte. Diese
geprunten Modelle konnten die höchsten Werte in den Evaluierungen erreichen. Das
rechen- und kostenintensive Fine-Tuning wurde daher nur testweise für das Modell
durchgeführt, das zu 30\% mit der \emph{Taylor}-Methode geprunt wurden.

Verwendet wurde dafür das im \emph{LLM-Pruner} vorhandene Skript
\emph{post\_training.py} über den folgenden Befehl:

\vspace{1em}
\begin{lstlisting}
$ python post_training.py 
    --prune_model prune_log/tinyllama_30_0418_prune_log/pytorch_model.bin
    --data_path yahma/alpaca-cleaned \
    --lora_r 8 \
    --num_epochs 2 \ 
    --learning_rate 1e-4 \ 
    --batch_size 64 \
    --output_dir tune_log/tinyllama_30_tuned \ 
    --wandb_project tinyllama_30_tune
\end{lstlisting}


\subsection{Evaluierung der Modelle}

Als Basis jeglicher durchgeführter Tests dienten jeweils die Ergebnisse des
selbst durchgeführten Benchmarks des TinyLlama-Basismodells. Alle erhobenen
Werte und Ergebnisse werden in Relation dazu bewertet.

\subsubsection{Modell-Benchmarks}

Die Modelle, die durch das Pruning und das anschließende Fine-Tuning entstanden
sind, wurden anschließend auf ihre verbliebenen Fähigkeiten hin untersucht. Zu
diesem Zweck wurden sie anhand verschiedener Datensätze evaluiert, die jeweils
unterschiedliche Aspekte der Leistungsfähigkeit von LLMs testen. Welche
spezifischen Aspekte dabei geprüft wurden, wurde bereits in den vorherigen
Abschnitten detailliert beschrieben.

\begin{itemize}
	\item{\emph{openbookqa}}
	\item{\emph{winogrande}}
	\item{\emph{hellaswag}}
	\item{\emph{arc\_challenge}}
	\item{\emph{boolq}}
\end{itemize}

Die Evaluierung anhand dieser Datensätze wurde, wie beim Pruning, mit dem immer
gleichen Befehl - angepasst an das jeweilige Modell - durchgeführt.

\vspace{1em}
\begin{lstlisting}
$ export PYTHONPATH='.'
$ python lm-evaluation-harness/main.py
    --model hf-causal-experimental
    --model_args checkpoint=[PATH_TO_MODEL]/pytorch_model.bin,
            config_pretrained=TinyLlama/TinyLlama-1.1B-Chat-v1.0
    --tasks openbookqa,winogrande,hellaswag,arc_challenge,boolq
    --device cuda:0
    --no_cache
    --output_path [PATH_TO_RESULTS]
\end{lstlisting}


Wie im Befehl ersichtlich, wurde das \emph{lm-evaluation-harness}-Framework von
OpenAI verwendet, das bereits im Repository enthalten ist. Dieses Framework
dient der Evaluierung von Sprachmodellen anhand verschiedener Benchmarks bzw.
\emph{Tasks}. Durch die Nutzung des Frameworks in Kombination mit den
definierten \emph{Tasks} wird eine standardisierte Bewertung der Modelle
ermöglicht, was wiederum den Vergleich unterschiedlicher Modelle erleichtert.

Das Argument \emph{--model hf-causal-experimental} wird übergeben, um die
Nutzung der GPU während der Tests zu gewährleisten. Zusätzlich ist die Angabe
des Pfads zum geprunten bzw. nachtrainierten Modell erforderlich, ebenso wie die
Grundarchitektur des Basismodells. Die im Befehl spezifizierten \emph{Tasks}
entsprechen dabei den zuvor beschriebenen.

Ein beispielhafter Aufruf zur Evaluierung des Modells, das zu 30\% geprunt wurde,
sieht wie folgt aus:

\vspace{1em}
\begin{lstlisting}
$ export PYTHONPATH='.'
$ python lm-evaluation-harness/main.py
    --model hf-causal-experimental
    --model_args checkpoint=prune_log/tinyllama_30_0418_l1_prune_log/pytorch_model.bin,
        config_pretrained=TinyLlama/TinyLlama-1.1B-Chat-v1.0
    --tasks openbookqa,winogrande,hellaswag,arc_challenge,boolq
    --device cuda:0
    --no_cache
    --output_path results/tinyllama_30_0418_l1
\end{lstlisting}

\subsubsection{Modellperfomance}

Neben den verbliebenen Fähigkeiten des geprunten LLMs soll zusätzlich dessen
Performance getestet werden. Die Messung der Performance erfolgt durch die
Durchführung der zuvor beschriebenen Benchmarks. Während dieser Tests werden
sowohl die benötigte Zeit als auch der in Anspruch genommene Speicher
berücksichtigt, um eine Bewertung der Effizienz des Modells zu ermöglichen.

Für diese Messungen wird der in der \emph{zsh}-Shell integrierte
\emph{time}-Befehl verwendet, der sowohl Angaben zur verbrauchten Zeit als auch
zum genutzten Speicher liefert. Um diese Daten zu erfassen, wird der
\emph{time}-Befehl einfach den zuvor beschriebenen Befehlen zur Durchführung der
Benchmarks vorangestellt. Ein entsprechender Aufruf zur Messung der Performance
sieht wie folgt aus:

\vspace{1em}
\begin{lstlisting}
$ export PYTHONPATH='.'
$ time python lm-evaluation-harness/main.py
    --model hf-causal-experimental
    ...
\end{lstlisting}

\newpage
