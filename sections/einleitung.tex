\section{Einleitung (Kai Herbst)}

\subsection{Problemstellung}

Mit dem berühmten Paper \emph{Attention is all you need}, in dem 2017 erstmals
die Transformer-Architektur beschrieben wurde, wurde der Grundstein für die
modernen Large Language Models (LLM) gelegt.
\autocite[Vgl.][]{vaswani2017attention} Nach weiteren Innovationen wie dem
Sprachmodell \emph{BERT} von Google, das erstmals tiefere Kontextverständnisse
ermöglichte\autocite[Vgl.][]{bertgoogle} fanden die LLMs spätestens mit der
Veröffentlichung von GPT-3, das mit 175 Milliarden Parametern einen großen
Sprung in der Sprachgenerierung machte, Einzug in Anwendungen vieler
verschiedener Bereiche.\autocite[Vgl.][]{gpt3} Diese Anzahl an verwendeter
Parametern und damit die Größe dieser Sprachmodelle steigt stetig an. Das
derzeit moderne GPT-4 soll laut verschiedener Quellen bereits mit ungefähr 1,8
Milliarden Parametern arbeiten.\autocite[Vgl.][]{gpt4size}

Das bedeutet jedoch, dass umso größer die Modelle werden umso mehr Parameter
beim Training angepasst werden müssen und die Berechnungen beziehungsweise die
Textgenerierung mit einem erheblichen Ressourcenverbrauch daherkommt. Abhilfe
soll hier das \emph{Pruning} dieser Modelle verschaffen, also das Entfernen von
Parametern möglichst ohne die Reduzierung der Performance der geprunten
Modelle.\autocite[Vgl.][]{llmpruner} Hierzu gibt es bereits verschiedene Ansätze
und Frameworks, die in dieser Hausarbeit betrachtet und erläutert werden sollen.
In einem eigenen praktischen Versuch soll dabei ein Large Language Model mit
einem dieser Frameworks geprunt und anschließend evaluiert werden.

\subsection{Forschungsfragen}

Die vorliegende Projektarbeit setzt sich mit insgesamt vier Forschungsfragen
auseinander, die im Verlauf der Arbeit untersucht und beantwortet werden sollen.
Die erste Forschungsfrage beschäftigt sich mit dem aktuellen Stand der Forschung
im Bereich des Prunings von Large Language Models (LLMs). Dabei wird analysiert,
welche Methoden derzeit für das Pruning angewendet werden, welche Unterschiede
zwischen diesen bestehen und welche Vor- und Nachteile die jeweiligen Ansätze
mit sich bringen. Das Ziel dieser Untersuchung ist es, einen Überblick über den
aktuellen Stand der Forschung in diesem Bereich zu geben und die relevantesten
Methoden zu beschreiben.

Ein weiterer Aspekt dieser Arbeit ist die Analyse der verschiedenen Frameworks,
die Pruning-Methoden für LLMs unterstützen. Hierbei soll untersucht werden,
welche dieser Frameworks welche Methoden implementieren und wie sie in der
Praxis angewendet werden können. Basierend auf dieser Analyse wird eine
geeignete Auswahl getroffen und eines dieser Framework für die praktische
Umsetzung festgelegt.

Neben der theoretischen Auseinandersetzung mit dem Thema wird zudem ein eigener
praktischer Versuch durchgeführt. Hierzu wird ein geeignetes Basismodell
ausgewählt, das anschließend in verschiedenen Stufen geprunt wird. Die
Auswirkungen dieses Prunings werden untersucht und dokumentiert. Im Rahmen
dieser Analyse werden standardisierte Benchmarks erstellt, um die geprunten
Modelle zu bewerten. Dabei wird nicht nur die Performance hinsichtlich der
Ergebnisse in den Tests betrachtet, sondern auch der Einfluss des Prunings auf
den Speicherverbrauch und die benötigte Rechenzeit. Ziel ist es, Erkenntnisse
darüber zu gewinnen, wie sich das Pruning auf verschiedene Aspekte der Modelle
auswirkt.

Zusammenfassend ergeben sich daraus die folgenden Forschungsfragen:

\begin{enumerate}
	\item Wie ist der aktuelle Stand der Forschung im Bereich des
	      Prunings von Large Language Models, und welche Methoden werden derzeit
	      eingesetzt?
	\item Welche Frameworks bieten Unterstützung für das Pruning von
	      LLMs, und wie lassen sich diese in der Praxis anwenden?
	\item Welche
	      Auswirkungen hat das Pruning eines Modells auf dessen Leistungsfähigkeit in
	      Bezug auf standardisierte Benchmarks und Aufgaben?
	\item Inwiefern
	      beeinflusst das Pruning eines Modells dessen Speicherverbrauch sowie die
	      benötigte Rechenzeit?
\end{enumerate}


\subsection{Aufbau der Arbeit}

Die Arbeit beschäftigt sich nachfolgend zunächst mit dem derzeitigen Stand der
Forschung im Bereich des Prunings von LLMs und den Möglichkeiten zur Evaluieren
deren Performances. Um die gewonnen Erkenntnisse in einem praktischen Versuch
selbst anwenden zu können, wird sich anschließend für eine geeignete Bibliothek
und ein geeignetes Large Language Model entschieden und darauf ein Pruning
unterschiedlicher Intensitäten durchgeführt. Das dafür verwendete Vorgehen wird
im Methodik-Teil genauer erläutert.

Von den geprunten Modellen wird eines nachtrainiert, mit dem Ziel die
Performance des reduzierten Modells zu verbessern.

Um die geprunten Modelle zu vergleichen, werden sie anhand verschiedener
Datensätze getestet. Die daraus entstehenden Resultate werden schließlich als
Grundlage für eine Diskussion verwendet.
