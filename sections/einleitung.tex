\section{Einleitung}

\subsection{Problemstellung}

Mit dem berühmten Paper \emph{Attention is all you need}, in dem 2017 erstmals
die Transformer-Architektur beschrieben wurde, wurde der Grundstein für die
modernen Large Language Models (LLM) gelegt.
\autocite[Vlg.][]{vaswani2017attention} 

\subsection{Forschungsfragen}

Die vorliegende Projektarbeit setzt sich mit insgesamt vier Forschungsfragen
auseinander, die im Verlauf der Arbeit untersucht und beantwortet werden sollen.
Die erste Forschungsfrage beschäftigt sich mit dem aktuellen Stand der Forschung
im Bereich des Prunings von Large Language Models (LLMs). Dabei wird analysiert,
welche Methoden derzeit für das Pruning angewendet werden, welche Unterschiede
zwischen diesen bestehen und welche Vor- und Nachteile die jeweiligen Ansätze
mit sich bringen. Das Ziel dieser Untersuchung ist es, einen Überblick über den
aktuellen Stand der Forschung in diesem Bereich zu geben und die relevantesten
Methoden zu beschreiben.

Ein weiterer Aspekt dieser Arbeit ist die Analyse der verschiedenen Frameworks,
die Pruning-Methoden für LLMs unterstützen. Hierbei soll untersucht werden,
welche dieser Frameworks welche Methoden implementieren und wie sie in der
Praxis angewendet werden können. Basierend auf dieser Analyse wird eine
geeignete Auswahl getroffen und eines dieser Framework für die praktische
Umsetzung festgelegt.

Neben der theoretischen Auseinandersetzung mit dem Thema wird zudem ein eigener
praktischer Versuch durchgeführt. Hierzu wird ein geeignetes Basismodell
ausgewählt, das anschließend in verschiedenen Stufen geprunt wird. Die
Auswirkungen dieses Prunings werden untersucht und dokumentiert. Im Rahmen
dieser Analyse werden standardisierte Benchmarks erstellt, um die geprunten
Modelle zu bewerten. Dabei wird nicht nur die Performance hinsichtlich der
Ergebnisse in den Tests betrachtet, sondern auch der Einfluss des Prunings auf
den Speicherverbrauch und die benötigte Rechenzeit. Ziel ist es, Erkenntnisse
darüber zu gewinnen, wie sich das Pruning auf verschiedene Aspekte der Modelle
auswirkt.

Zusammenfassend ergeben sich daraus die folgenden Forschungsfragen:

\begin{enumerate}
	\item Wie ist der aktuelle Stand der Forschung im Bereich des
	      Prunings von Large Language Models, und welche Methoden werden derzeit
	      eingesetzt?
	\item Welche Frameworks bieten Unterstützung für das Pruning von
	      LLMs, und wie lassen sich diese in der Praxis anwenden?
	\item Welche
	      Auswirkungen hat das Pruning eines Modells auf dessen Leistungsfähigkeit in
	      Bezug auf standardisierte Benchmarks und Aufgaben?
	\item Inwiefern
	      beeinflusst das Pruning eines Modells dessen Speicherverbrauch sowie die
	      benötigte Rechenzeit?
\end{enumerate}


\subsection{Aufbau der Arbeit}

Die Arbeit beschäftigt sich nachfolgend zunächst mit dem derzeitigen Stand der
Forschung im Bereich des Prunings von LLMs und den Möglichkeiten zur Evaluieren
deren Performances. Um die gewonnen Erkenntnisse in einem praktischen Versuch
selbst anwenden zu können, wird sich anschließend für eine geeignete Bibliothek
und ein geeignetes Large Language Model entschieden und darauf ein Pruning
unterschiedlicher Intensitäten durchgeführt. Das dafür verwendete Vorgehen wird
im Methodik-Teil genauer erläutert.

Von den geprunten Modellen wird eines nachtrainiert, mit dem Ziel die
Performance des reduzierten Modells zu verbessern.

Um die geprunten Modelle zu vergleichen, werden sie anhand verschiedener
Datensätze getestet. Die daraus entstehenden Resultate werden schließlich als
Grundlage für eine Diskussion verwendet.
